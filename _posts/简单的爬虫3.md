---
title: 简单的python爬虫 (三)
tag: python
thumbnail: https://pic.kylinn.cloud/uploads/big/e33dafe5e69bef97839c1bac1f9d45b6.jpg
date: 2021/4/29
---

## 爬虫框架
上文简单说了一下爬到网页之后的内容提取，但是感觉十分麻烦。[上文链接](https://kylinn.cloud/2021/04/26/%E7%AE%80%E5%8D%95%E7%9A%84%E7%88%AC%E8%99%AB2)
所以再正式开始爬取图片之前，我先介绍一下 **爬虫框架**，这里要说的是 **BeautifulSoup4**。

别的先不说，直接看结果:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200611202852780.png#pic_center)

再看代码，你就知道用框架有多方便，其实bs4可能不该叫框架 👀 。

```python
import requests as rq
import re
from bs4 import BeautifulSoup as bs
from bs4 import SoupStrainer as ss

url = 'http://wjw.ah.gov.cn/xwzx/gzdt/53195761.html'
html = rq.get(url)
html.encoding = 'utf-8'
status = html.status_code
content = html.text
print(status)

need = ss(class_='wzcon j-fontContent clearfix')
soup = bs(content, 'lxml', parse_only=need)
print(soup.get_text())

```
对的，只有三句话。底层已经封装的很好了，我们直接调用就行。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200611203301225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t5bGluaG9sbWVz,size_16,color_FFFFFF,t_70#pic_center)

- 先找到中间那段正文那里，发现 class 是 'wzcon j-fontContent clearfix'，我们也只要这一部分的内容，所以用 **SoupStrainer** 提取我们需要的那一段。注意参数是 `class_` 有下划线
- 写法是把上文的 need 作为 **BeautifulSoup** 中 parse_only 的参数。需要注意的是'lxml'这个是处理 html/xml 的底层，默认写这个就行了，其他的解析器不再介绍。
- soup 就是我们拿来用的东西，读者可以自己输出来看一下，soup内容是什么，这里就不输出了。
- 直接 get_text() 函数 可以获取到 **文本** 。

BeautifulSoup4 安装
```python
pip3 install beautifulsoup4
```
lxml 安装

```python
pip3 install lxml
```
BeautifulSoup知识点比较多，我不做多的介绍，感兴趣可以自行百度，有官方中文。
## 图片爬虫
这里我们开始爬取图片教程：
下面这个是我们要爬取的网站，但是出于**某些考虑**，就把网址部分打码了，相关字段都删了。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200611211718327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t5bGluaG9sbWVz,size_16,color_FFFFFF,t_70#pic_center)

## 网页分析

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200611211855112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t5bGluaG9sbWVz,size_16,color_FFFFFF,t_70#pic_center)

找到我们需要的那一部分，发现每张图片都有个链接，就是那些a标签，直接打开就可以查看到图片，就是下面的样子了

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200611212345295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t5bGluaG9sbWVz,size_16,color_FFFFFF,t_70#pic_center)

那就很简单了，我们找到所有的 a标签 ，把里面的内容保存下来就是图片了。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200611213900902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t5bGluaG9sbWVz,size_16,color_FFFFFF,t_70#pic_center)

## 代码解释
因为大体上都差不多，我就只解释新的内容：
```python
tag = soup.find_all('a')
```
这句话是，找到soup里全部的a标签，返回对象是一个list，每个list的成员是一个tag类型的变量，tag就是说的 网页里的标签嘛 🏷️ 

`tag`对象可以当成字典来用
这样就可以查看到`href`属性的值，也就是图片的链接
```python
tag[0]['href']
# https://nmj.miaowugo.com/wp-content/uploads/2020/06/58290-1591888143.jpg?x-oss-process=image/resize,m_fill,h_1536,w_1024
```
通过`attrs` 可以知道有哪些 `key`
```python
print(tag[0].attrs)
# {'href': 'https://nmj.miaowugo.com/wp-content/uploads/2020/06/58290-1591888143.jpg?x-oss-process=image/resize,m_fill,h_1536,w_1024', 'rel': ['attachment', 'wp-att-58291']}
```
## 全部代码
```python
import requests as rq
import re
from bs4 import BeautifulSoup as bs
from bs4 import SoupStrainer as ss


def save_img(url, name):
# 保存图片的函数，就不解释这个了
    h = rq.get(url)
    with open(name, 'wb+') as f:
        f.write(h.content)
    return


url = 'https://xxxxx'
# 先前写的时候，我还记着说要删掉这里，结果发了好久突然想到没有删。
html = rq.get(url)
html.encoding = 'utf-8'
status = html.status_code
content = html.text
print(status)

need = ss(class_='nv-content-wrap entry-content')
soup = bs(content, 'lxml', parse_only=need)

tag = soup.find_all('a')
# 找到所有的a标签，返回一个 tag类型的 list 给 tag变量
j = 0 # 当名字用的变量
for i in tag:
    print(i['href'])
    save_img(str(i['href']), str(j)+'.jpg')
    j += 1
print('OK')

```
## 后记
**BeautifulSoup**用着还是蛮方便的，我没有详细的讲这个库，有需要的读者可以自行查阅，我也是刚刚才看了回来。有官方中文，NICE！

爬虫的内容其实我都没有讲很多，比如反爬技术、Header、代理池等，我觉得这些说起来比较多，简单爬虫不合适，就点到为止吧。
